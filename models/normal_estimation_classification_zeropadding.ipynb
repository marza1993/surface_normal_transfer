{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"normal_estimation_classification_zeropadding.ipynb","provenance":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"cXl2LIsHBk9A","colab_type":"text"},"source":["#Installazione pacchetti necessari"]},{"cell_type":"code","metadata":{"id":"NYFZ_SV6Bw30","colab_type":"code","outputId":"d1be98d5-727b-4b84-9a3a-9a89b17916fd","executionInfo":{"status":"ok","timestamp":1588257971013,"user_tz":-120,"elapsed":18089,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Install required libs\n","\n","### please update Albumentations to version>=0.3.0 for `Lambda` transform support\n","!pip install -U albumentations==0.3.0 --user \n","!pip install -U --pre segmentation-models --user\n","#!pip install git+https://github.com/qubvel/segmentation_models --user"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting albumentations==0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/e9/f9bc651776d286bf7a22e2c4685d44a5f565ed0ad27e988b1d7bdb9875c7/albumentations-0.3.0.tar.gz (74kB)\n","\r\u001b[K     |████▍                           | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 20kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 30kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 71kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.0) (1.18.3)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.0) (1.4.1)\n","Collecting opencv-python-headless\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/2c/909a04b07360516953beaf6f66480bb6b84b817c6b300c1235bfb2901ad8/opencv_python_headless-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (21.6MB)\n","\u001b[K     |████████████████████████████████| 21.6MB 1.4MB/s \n","\u001b[?25hCollecting imgaug<0.2.7,>=0.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n","\u001b[K     |████████████████████████████████| 634kB 48.3MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.0) (3.13)\n","Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (0.16.2)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (1.12.0)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.4)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (1.1.1)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (3.2.1)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.4.1)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (7.0.0)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (4.4.2)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.8.1)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (0.10.0)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (1.2.0)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.4.7)\n","Building wheels for collected packages: albumentations, imgaug\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-0.3.0-cp36-none-any.whl size=46998 sha256=3f65a5a753168a4d030ba2d86ec31ec9835a54894db849c419d9e8389e56f401\n","  Stored in directory: /root/.cache/pip/wheels/f0/20/08/c30ec4f36c3abe52bfb5a980434d4017776f17266071425c9b\n","  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=4175275c3721526d52681e32d239b95f458c5e9cf36b238da9897a18a005d3ec\n","  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n","Successfully built albumentations imgaug\n","Installing collected packages: opencv-python-headless, imgaug, albumentations\n","Successfully installed albumentations-0.3.0 imgaug-0.2.6 opencv-python-headless-4.2.0.34\n","Collecting segmentation-models\n","  Downloading https://files.pythonhosted.org/packages/da/b9/4a183518c21689a56b834eaaa45cad242d9ec09a4360b5b10139f23c63f4/segmentation_models-1.0.1-py3-none-any.whl\n","Collecting image-classifiers==1.0.0\n","  Downloading https://files.pythonhosted.org/packages/81/98/6f84720e299a4942ab80df5f76ab97b7828b24d1de5e9b2cbbe6073228b7/image_classifiers-1.0.0-py3-none-any.whl\n","Collecting efficientnet==1.0.0\n","  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.8)\n","Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.16.2)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.18.3)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (2.10.0)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.1)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.0.0)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.1.1)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.12.0)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models) (4.4.2)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.10.0)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.2.0)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.7)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.1)\n","Installing collected packages: image-classifiers, efficientnet, segmentation-models\n","Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 segmentation-models-1.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JVK2YhY_fQXT","colab_type":"text"},"source":["# Connessione a directory Drive"]},{"cell_type":"code","metadata":{"id":"i6Nt5L3oermW","colab_type":"code","outputId":"cd6bf1a0-ec83-4a3b-866b-c7997f0e4ab0","executionInfo":{"status":"ok","timestamp":1588258057010,"user_tz":-120,"elapsed":21597,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JKAzI_xNfUWB","colab_type":"text"},"source":["# Loading Dataset immagini di input"]},{"cell_type":"code","metadata":{"id":"THTHHUIVfPUP","colab_type":"code","colab":{}},"source":["import numpy as np\n","PATH_BASE = '/content/drive/My Drive/Appunti delle lezioni/2Anno2Semestre/Digital Image Processing/surface_normal_estimation_us/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gE2iJUt0faRn","colab_type":"code","colab":{}},"source":["immagini_db = np.load(PATH_BASE + 'input_imgs_dataset.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"guc-sX0Nfka-","colab_type":"code","outputId":"56b8943f-6fe4-45d6-8c03-49ff7bf34b25","executionInfo":{"status":"ok","timestamp":1588258071108,"user_tz":-120,"elapsed":1097,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(immagini_db.shape)\n","N = immagini_db.shape[0]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1446, 195, 260, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"odQAMNg4-ALu","colab_type":"text"},"source":["# Zero padding per estensione superficie immagini"]},{"cell_type":"code","metadata":{"id":"3kJCVSSW-DnX","colab_type":"code","outputId":"476a0326-bb4a-40a0-d2c7-6ad69ddb2842","executionInfo":{"status":"error","timestamp":1588230337167,"user_tz":-120,"elapsed":8526,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["immagini_db_zeropadding = np.zeros((immagini_db.shape[0],320,320,3), dtype=immagini_db.dtype) \n","for img in range(0, immagini_db.shape[0]):\n","  for width in range(0,immagini_db.shape[1]):\n","    for height in range(0,immagini_db.shape[2]):\n","      for channel in range(0,immagini_db.shape[3]):\n","        immagini_db_zeropadding[img][width][height][channel] = immagini_db[img][width][height][channel]"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-c646f2ef94d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimmagini_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mchannel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimmagini_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mimmagini_db_zeropadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimmagini_db\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"b6KVzBP5AxAZ","colab_type":"code","outputId":"c8bf0e4a-1a1d-4f72-b271-0281e920280e","executionInfo":{"status":"ok","timestamp":1588151947546,"user_tz":-120,"elapsed":1335,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(immagini_db_zeropadding.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1446, 320, 320, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w7BBAIw-BLQw","colab_type":"code","colab":{}},"source":["np.save(PATH_BASE + \"input_imgs_dataset_zeropadding.npy\", immagini_db_zeropadding)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHcp0yHDA5XB","colab_type":"code","colab":{}},"source":["immagini_db_zeropadding = np.load(PATH_BASE + \"input_imgs_dataset_zeropadding.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wr8u_zaNf6Lt","colab_type":"text"},"source":["# Loading Clusterizzazione in 40 normali pixel per pixel"]},{"cell_type":"code","metadata":{"id":"2Eytvr3ogH4c","colab_type":"code","colab":{}},"source":["reshaped_labels = np.load(PATH_BASE + \"normals_centroid_labels.npy\")\n","codebook = np.load(PATH_BASE + \"codebook_labels_3d_components.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IJYl1XRy1AFQ","colab_type":"code","outputId":"a4afac20-cf36-44f9-aa71-2d21e10843d8","executionInfo":{"status":"ok","timestamp":1587913741689,"user_tz":-120,"elapsed":712,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["reshaped_labels.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1446, 195, 260)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"3fIUXHoAPeSO","colab_type":"text"},"source":["## Zero padding anche sulle label"]},{"cell_type":"code","metadata":{"id":"Z8eQhVOWPgpc","colab_type":"code","colab":{}},"source":["reshaped_labels_zeropadding = np.full((reshaped_labels.shape[0], 320, 320), 40)\n","for n in range(0,reshaped_labels.shape[0]):\n","  for width in range(0, reshaped_labels.shape[1]):\n","    for height in range(0, reshaped_labels.shape[2]):\n","      reshaped_labels_zeropadding[n][width][height] = reshaped_labels[n][width][height]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vHhA29xg41cE","colab_type":"code","colab":{}},"source":["np.save(PATH_BASE + \"normals_centroid_labels_zeropadding.npy\", reshaped_labels_zeropadding)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQO_mtTg4uOs","colab_type":"code","colab":{}},"source":["reshaped_labels_zeropadding = np.load(PATH_BASE + \"normals_centroid_labels_zeropadding.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dkifSxZGPhN_","colab_type":"text"},"source":["## Passaggio a 41 channel"]},{"cell_type":"markdown","metadata":{"id":"urThIMc80uxG","colab_type":"text"},"source":["Si passa da WxH a WxHxC dove C rappresenta i singoli cluster"]},{"cell_type":"code","metadata":{"id":"7Oz6DNaI06gX","colab_type":"code","colab":{}},"source":["def process_channels(reshaped_labels, n_labels):\n","  reshaped_labels_processed = np.zeros((reshaped_labels.shape[0], reshaped_labels.shape[1], reshaped_labels.shape[2], n_labels), dtype=\"uint8\")\n","  for n in range(0, reshaped_labels_processed.shape[0]):\n","    for i in range(0, reshaped_labels_processed.shape[1]):\n","      for j in range(0, reshaped_labels_processed.shape[2]):\n","        reshaped_labels_processed[n][i][j][reshaped_labels[n][i][j]] = 1\n","  return reshaped_labels_processed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUYyjbUcRNdE","colab_type":"code","colab":{}},"source":["reshaped_labels_processed_zeropadding = process_channels(reshaped_labels_zeropadding, 41)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfnB1bQy8lCO","colab_type":"code","colab":{}},"source":["np.save(PATH_BASE + \"normals_centroid_labels_41channels.npy\", reshaped_labels_processed_zeropadding)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxLK-vw5_P__","colab_type":"code","colab":{}},"source":["reshaped_labels_processed_zeropadding = np.load(PATH_BASE + \"normals_centroid_labels_41channels.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0vrPAlXTgViQ","colab_type":"text"},"source":["# Modellizzazione"]},{"cell_type":"code","metadata":{"id":"XbeGcEY8h6SE","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","x_trainval, x_test, y_trainval, y_test = train_test_split(immagini_db_zeropadding, reshaped_labels_processed_zeropadding, test_size=0.3, random_state=1221)\n","x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.2, random_state=2442)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CcRpTOmTjfwQ","colab_type":"code","outputId":"82ab2c49-7f42-42e8-b7da-8ebc222d9b4c","executionInfo":{"status":"ok","timestamp":1588258321066,"user_tz":-120,"elapsed":6445,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import cv2\n","import tensorflow as tf\n","import keras\n","#import tensorflow.keras\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"31NF5OukjlNv","colab_type":"code","colab":{}},"source":["# helper function for data visualization\n","def visualize(**images):\n","    \"\"\"PLot images in one row.\"\"\"\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()\n","    \n","# helper function for data visualization    \n","def denormalize(x):\n","    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n","    x_max = np.percentile(x, 98)\n","    x_min = np.percentile(x, 2)    \n","    x = (x - x_min) / (x_max - x_min)\n","    x = x.clip(0, 1)\n","    return x\n","    \n","\n","# classes for data loading and preprocessing\n","class Dataset:\n","    \"\"\"Normal surface dataset. Read images, apply augmentation and preprocessing transformations.\n","    \n","    Args:\n","        x (nparray): images\n","        y (nparray): label\n","        augmentation (albumentations.Compose): data transfromation pipeline \n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing \n","            (e.g. noralization, shape manipulation, etc.)\n","    \n","    \"\"\"\n","    \n","    def __init__(\n","            self, \n","            x, \n","            y, \n","            augmentation=None, \n","            preprocessing=None,\n","    ):\n","        self.x = x\n","        self.y = y\n","        \n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","    \n","    def __getitem__(self, i):\n","        # read data\n","        image = self.x[i]\n","        label = self.y[i]\n","        \n","        # apply augmentations\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, label=label)\n","            image, mask = sample['image'], sample['label']\n","        \n","        # apply preprocessing\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, label=label)\n","            image, mask = sample['image'], sample['label']\n","            \n","        return image, label\n","        \n","    def __len__(self):\n","        return self.x.shape[0]\n","    \n","    \n","class Dataloder(keras.utils.Sequence):\n","    \"\"\"Load data from dataset and form batches\n","    \n","    Args:\n","        dataset: instance of Dataset class for image loading and preprocessing.\n","        batch_size: Integet number of images in batch.\n","        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n","    \"\"\"\n","    \n","    def __init__(self, dataset, batch_size=1, shuffle=False):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.indexes = np.arange(len(dataset))\n","\n","        self.on_epoch_end()\n","\n","    def __getitem__(self, i):\n","        \n","        # collect batch data\n","        start = i * self.batch_size\n","        stop = (i + 1) * self.batch_size\n","        data = []\n","        for j in range(start, stop):\n","            data.append(self.dataset[j])\n","        \n","        # transpose list of lists\n","        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n","        \n","        return batch\n","    \n","    def __len__(self):\n","        \"\"\"Denotes the number of batches per epoch\"\"\"\n","        return len(self.indexes) // self.batch_size\n","    \n","    def on_epoch_end(self):\n","        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n","        if self.shuffle:\n","            self.indexes = np.random.permutation(self.indexes)   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yyD_fERaLQp3","colab_type":"code","colab":{}},"source":["import albumentations as A\n","\n","def round_clip_0_1(x, **kwargs):\n","    return x.round().clip(0, 1)\n","\n","# define heavy augmentations\n","def get_training_augmentation():\n","    train_transform = [\n","\n","        A.HorizontalFlip(p=0.5),\n","\n","        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n","\n","        A.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n","        A.RandomCrop(height=320, width=320, always_apply=True),\n","\n","        A.IAAAdditiveGaussianNoise(p=0.2),\n","        A.IAAPerspective(p=0.5),\n","\n","        A.OneOf(\n","            [\n","                A.CLAHE(p=1),\n","                A.RandomBrightness(p=1),\n","                A.RandomGamma(p=1),\n","            ],\n","            p=0.9,\n","        ),\n","\n","        A.OneOf(\n","            [\n","                A.IAASharpen(p=1),\n","                A.Blur(blur_limit=3, p=1),\n","                A.MotionBlur(blur_limit=3, p=1),\n","            ],\n","            p=0.9,\n","        ),\n","\n","        A.OneOf(\n","            [\n","                A.RandomContrast(p=1),\n","                A.HueSaturationValue(p=1),\n","            ],\n","            p=0.9,\n","        ),\n","        A.Lambda(mask=round_clip_0_1)\n","    ]\n","    return A.Compose(train_transform)\n","\n","\n","def get_validation_augmentation():\n","    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n","    test_transform = [\n","        A.PadIfNeeded(384, 480)\n","    ]\n","    return A.Compose(test_transform)\n","\n","def get_preprocessing(preprocessing_fn):\n","    \"\"\"Construct preprocessing transform\n","    \n","    Args:\n","        preprocessing_fn (callbale): data normalization function \n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose\n","    \n","    \"\"\"\n","    \n","    _transform = [\n","        A.Lambda(image=preprocessing_fn),\n","    ]\n","    return A.Compose(_transform)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJeP8VH5kFzt","colab_type":"text"},"source":["Costruzione modello"]},{"cell_type":"code","metadata":{"id":"fvSmZ724kA38","colab_type":"code","outputId":"56f963ef-747a-4b37-ab66-19a14cbc842c","executionInfo":{"status":"ok","timestamp":1588258327817,"user_tz":-120,"elapsed":1073,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import segmentation_models as sm\n","\n","# segmentation_models could also use `tf.keras` if you do not have Keras installed\n","# or you could switch to other framework using `sm.set_framework('tf.keras')`"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Segmentation Models: using `keras` framework.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8fDpbHNpkJ5o","colab_type":"code","colab":{}},"source":["BACKBONE = 'efficientnetb3'\n","BATCH_SIZE = 8\n","LR = 0.0001\n","EPOCHS = 40\n","\n","preprocess_input = sm.get_preprocessing(BACKBONE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdfO0YcOkKjz","colab_type":"code","outputId":"0ce17cf6-cf63-4aa7-c214-29d2fc44bb68","executionInfo":{"status":"ok","timestamp":1588258347214,"user_tz":-120,"elapsed":17420,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["# define network parameters\n","n_classes = 41\n","activation = 'softmax'\n","\n","#create model\n","model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n","44113920/44107200 [==============================] - 2s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gyH57h0w-eOC","colab_type":"text"},"source":["Calcolo dei pesi"]},{"cell_type":"code","metadata":{"id":"cd13xtLxkYHb","colab_type":"code","colab":{}},"source":["from sklearn.utils import class_weight\n","def computeLabelWeights(reshaped_labels):\n","  labelList = []\n","  for n in range(0, reshaped_labels.shape[0]):\n","    for i in range(0, reshaped_labels.shape[1]):\n","      for j in range(0, reshaped_labels.shape[2]):\n","        labelList.append(reshaped_labels[n][i][j])\n","\n","  return class_weight.compute_class_weight('balanced',np.unique(labelList),labelList).tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQm8-YOEQD2u","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXr1icB293JU","colab_type":"code","colab":{}},"source":["class_weight = computeLabelWeights(reshaped_labels_zeropadding)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O2A-_vKA-hfX","colab_type":"code","colab":{}},"source":["np.save(PATH_BASE + \"label_weights_zeropadding.npy\", class_weight)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2PmFmpn-x0Z","colab_type":"code","colab":{}},"source":["class_weight = np.load(PATH_BASE + \"label_weights_zeropadding.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuoV1EQVkw6b","colab_type":"code","colab":{}},"source":["# define optomizer\n","optim = keras.optimizers.Adam(LR)\n","\n","# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n","# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n","dice_loss = sm.losses.DiceLoss(class_weights=class_weight)\n","focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n","total_loss = dice_loss + (1 * focal_loss)\n","\n","# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n","# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n","\n","metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n","\n","# compile keras model with defined optimozer, loss and metrics\n","model.compile(optim, total_loss, metrics)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Hky3fONufbf","colab_type":"code","colab":{}},"source":["# Dataset for train images\n","train_dataset = Dataset(\n","    x_train, \n","    y_train\n","    #augmentation=get_training_augmentation(),\n","    #preprocessing=get_preprocessing(preprocess_input),\n",")\n","\n","# Dataset for validation images\n","valid_dataset = Dataset(\n","    x_val, \n","    y_val\n","    #augmentation=get_validation_augmentation(),\n","    #preprocessing=get_preprocessing(preprocess_input),\n",")\n","\n","train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)\n","\n","# check shapes for errors\n","assert train_dataloader[0][0].shape == (BATCH_SIZE, 320, 320, 3)\n","assert train_dataloader[0][1].shape == (BATCH_SIZE, 320, 320, 41)\n","\n","# define callbacks for learning rate scheduling and best checkpoints saving\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\n","    keras.callbacks.ReduceLROnPlateau(),\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0pkZ5oBgrQm7","colab_type":"code","outputId":"01724d5d-8ec1-403b-d8c0-1dbe014eaa97","executionInfo":{"status":"error","timestamp":1588280113714,"user_tz":-120,"elapsed":2011,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["# train model\n","history = model.fit_generator(\n","    train_dataloader, \n","    steps_per_epoch=len(train_dataloader), \n","    epochs=EPOCHS, \n","    callbacks=callbacks, \n","    validation_data=valid_dataloader, \n","    validation_steps=len(valid_dataloader),\n",")"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fc2eae185d4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit_generator(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"_BCjl15PJP0t","colab_type":"code","outputId":"54246ef1-fb6a-43c6-d7b5-35389effce7b","executionInfo":{"status":"error","timestamp":1588232180798,"user_tz":-120,"elapsed":12818,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":508}},"source":["model.fit(\n","   x=x_train,\n","   y=y_train,\n","   batch_size=16,\n","   epochs=100,\n","   validation_data=(x_val, y_val),\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 809 samples, validate on 203 samples\n","Epoch 1/100\n"],"name":"stdout"},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-ee66202ccfac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[16,41,320,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradients/loss/softmax_loss/dice_loss_plus_1focal_loss/Pow_grad/mul (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_81795]\n\nFunction call stack:\nkeras_scratch_graph\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ez6sLWAggXI2","colab_type":"text"},"source":["# Ricalcolo superfici normali da classificazione"]},{"cell_type":"code","metadata":{"id":"3gshBXstgX3Y","colab_type":"code","colab":{}},"source":["# Metodo che permette di ottenere la normale 3d per ogni pixel a partire dalla predizione della rete.\n","# INPUT\n","# netPrediction: shape = (H, W, 40). Predizione della rete di un'immagine. \n","#                Per ogni pixel la rete fornisce un vettore di 40 probabilità (1 per ogni centroide).\n","# codebook:      shape = (40,3). Lista delle 40 normali di riferimento (centroidi). codebook.shape = (40,3)\n","# triangoli:     shape = (N, 3). Triangolazione di delaunay; ogni elemento di questo vettore è una tripla di indici di centroidi, da utilizzare\n","#                nel codebook ottenuto con la clusterizzazione. (es. N = 67)\n","# OUTPUT\n","# norm:          shape = (H, W, 3). Normali 3d pixel per pixel dell'immagine.\n","\n","def decode(netPrediction, codebook, triangoli):\n","  h, w = netPrediction.shape[0:2]\n","  \n","  # for each triangle, get total prob\n","  tri_prob = np.dstack([np.sum(netPrediction[:,:,t], axis = -1) for t in triangoli]) \n","\n","  # get best tri\n","  best_tri = np.argmax(tri_prob, axis = -1)\n","  \n","  # get coefficients to most probable tri\n","  alphas = np.reshape([netPrediction[row, col, triangoli[best_tri[row, col]]] for row in range(h) for col in range(w)], (h, w, 3))\n","\n","  # sum -> 1\n","  alphas = np.divide(alphas, np.reshape(np.tile(np.sum(alphas, axis = -1), (1,3)), (h, w, 3)))\n","  alphas = np.reshape(np.tile(alphas, (1,1,3)), (h, w,3,3))  \n","  alphas = np.swapaxes(alphas, 2,3)\n","\n","  norm = np.multiply(alphas, codebook[triangoli[best_tri]])\n","  norm = np.sum(norm, axis = -2)\n","  \n","  return norm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3pQS4nYKgzK1","colab_type":"code","colab":{}},"source":["# chiamo la funzione di decodifica e ottengo la mappa delle normali 3d\n","netPrediction_3d = decode(netPrediction_mock, codebook, triangoli)\n","print(netPrediction_3d.shape)\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","#plt.figure('es. predizione decodificata')\n","plt.imshow(255/2 * netPrediction_3d)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWaT7KxAhNTD","colab_type":"text"},"source":["# Valutazione"]},{"cell_type":"code","metadata":{"id":"XAhRq-5NhbO5","colab_type":"code","colab":{}},"source":["# pixel wise evaluation\n","# INPUT\n","# prediction:   shape = (1,3) normale predetta dalla rete (ottenuta come codifica dalle label)\n","# ground_truth: shape = (1,3) normale effettiva\n","# OUTPUT\n","# l'errore in radianti tra la normale passata e quella effettiva\n","def eval_cosine(prediction, ground_truth):\n","  if np.sum(ground_truth) == 0 or np.sum(prediction) == 0:\n","    return 0\n","  cosine = np.dot(prediction, ground_truth) / (np.linalg.norm(prediction) * np.linalg.norm(ground_truth))\n","  if (cosine <= 1 and cosine >= -1):\n","    return np.arccos(cosine) \n","  return np.deg2rad(180) if cosine < -1 else 0\n","\n","# questa funzione va chiamata su tutti i pixel di tutte le immagini del test-set o validation set."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DffV2pi-hdJj","colab_type":"code","colab":{}},"source":["# TODO: effettuare la predizione su tutto il test-set (es: 30% del dataset \"normals_orig.npy\") con la rete. \n","# Dopodichè decodificare le predizioni per ogni immagine (label -> normali 3d) e possibilmente salvare in un file (es: \"prediction_test_decoded.npy\").\n","\n","# es. di test-set\n","idx_test = np.random.randint(0, N, int(0.3 * N))\n","normali_test = normali[idx_test]\n","\n","# NOTA: nel calcolo della metrica non vanno considerati i pixel rumorosi, che sono identificati dalla tripla [0,0,0].\n","tot_noised_pixel = np.where(np.sum(normali_test, axis = -1) == 0)[0].shape[0]\n","tot_pixel = normali_test.shape[0] * normali_test.shape[1] * normali_test.shape[2]\n","valid_pixel = tot_pixel - tot_noised_pixel\n","\n","print('Noised pixel / Tot pixel (%):', tot_noised_pixel / tot_pixel * 100)\n","print('Mean: ', np.rad2deg(np.sum(theta) / valid_pixel))\n","\n","# es: carico da file\n","# theta = np.load(\"prediction_test_decoded.npy\")\n","\n","# valuto la percentuale di errori angolari inferiori rispetto ad una delle seguenti soglie\n","soglie_errori = [11.25, 22.5, 30]\n","for th in soglie_errori:\n","  under_th = (np.where(theta < np.deg2rad(th))[0].shape[0] - noised_pixel) / valid_pixel * 100\n","  print('Threshold', str(th),  under_th)"],"execution_count":0,"outputs":[]}]}