{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"normal_estimation_classification_zeropadding_preload.ipynb","provenance":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"cXl2LIsHBk9A","colab_type":"text"},"source":["#Installazione pacchetti necessari"]},{"cell_type":"code","metadata":{"id":"NYFZ_SV6Bw30","colab_type":"code","outputId":"c675bef3-d8c4-491a-aae0-bbf18525ac6b","executionInfo":{"status":"ok","timestamp":1588315446818,"user_tz":-120,"elapsed":17069,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Install required libs\n","\n","### please update Albumentations to version>=0.3.0 for `Lambda` transform support\n","!pip install -U albumentations==0.3.0 --user \n","!pip install -U --pre segmentation-models --user\n","#!pip install git+https://github.com/qubvel/segmentation_models --user"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting albumentations==0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/e9/f9bc651776d286bf7a22e2c4685d44a5f565ed0ad27e988b1d7bdb9875c7/albumentations-0.3.0.tar.gz (74kB)\n","\r\u001b[K     |████▍                           | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.6MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.0) (1.18.3)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.0) (1.4.1)\n","Collecting opencv-python-headless\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/2c/909a04b07360516953beaf6f66480bb6b84b817c6b300c1235bfb2901ad8/opencv_python_headless-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (21.6MB)\n","\u001b[K     |████████████████████████████████| 21.6MB 1.5MB/s \n","\u001b[?25hCollecting imgaug<0.2.7,>=0.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n","\u001b[K     |████████████████████████████████| 634kB 51.1MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.0) (3.13)\n","Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (0.16.2)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (1.12.0)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (1.1.1)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.4)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (7.0.0)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.4.1)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (3.2.1)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (4.4.2)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (0.10.0)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (1.2.0)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.8.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.4.7)\n","Building wheels for collected packages: albumentations, imgaug\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-0.3.0-cp36-none-any.whl size=46998 sha256=0d80913813e895fde5ebad66bc37b745d95ac96cfb32d48f1504c2cecf6b6a18\n","  Stored in directory: /root/.cache/pip/wheels/f0/20/08/c30ec4f36c3abe52bfb5a980434d4017776f17266071425c9b\n","  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=843bd008f0f4eafd3e3821036ad8e8c4cba5256c1ea9bda03d5665165a32da4b\n","  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n","Successfully built albumentations imgaug\n","Installing collected packages: opencv-python-headless, imgaug, albumentations\n","Successfully installed albumentations-0.3.0 imgaug-0.2.6 opencv-python-headless-4.2.0.34\n","Collecting segmentation-models\n","  Downloading https://files.pythonhosted.org/packages/da/b9/4a183518c21689a56b834eaaa45cad242d9ec09a4360b5b10139f23c63f4/segmentation_models-1.0.1-py3-none-any.whl\n","Collecting efficientnet==1.0.0\n","  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n","Collecting image-classifiers==1.0.0\n","  Downloading https://files.pythonhosted.org/packages/81/98/6f84720e299a4942ab80df5f76ab97b7828b24d1de5e9b2cbbe6073228b7/image_classifiers-1.0.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.8)\n","Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.16.2)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.18.3)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (2.10.0)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.1.1)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.1)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.0.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.12.0)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models) (4.4.2)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.7)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.2.0)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.10.0)\n","Installing collected packages: efficientnet, image-classifiers, segmentation-models\n","Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 segmentation-models-1.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JVK2YhY_fQXT","colab_type":"text"},"source":["# Connessione a directory Drive"]},{"cell_type":"code","metadata":{"id":"i6Nt5L3oermW","colab_type":"code","outputId":"1409d12d-b418-4280-9b16-63d5dcdfedb0","executionInfo":{"status":"ok","timestamp":1588324847027,"user_tz":-120,"elapsed":579,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JKAzI_xNfUWB","colab_type":"text"},"source":["# Loading Dataset immagini di input"]},{"cell_type":"code","metadata":{"id":"THTHHUIVfPUP","colab_type":"code","colab":{}},"source":["import numpy as np\n","PATH_BASE = '/content/drive/My Drive/Appunti delle lezioni/2Anno2Semestre/Digital Image Processing/surface_normal_estimation_us/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gE2iJUt0faRn","colab_type":"code","colab":{}},"source":["immagini_db = np.load(PATH_BASE + 'input_imgs_dataset.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"guc-sX0Nfka-","colab_type":"code","outputId":"7f669da8-ce68-49ce-bad4-76171df8e096","executionInfo":{"status":"ok","timestamp":1588315496010,"user_tz":-120,"elapsed":9235,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(immagini_db.shape)\n","N = immagini_db.shape[0]"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(1446, 195, 260, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"odQAMNg4-ALu","colab_type":"text"},"source":["# Zero padding per estensione superficie immagini"]},{"cell_type":"code","metadata":{"id":"aHcp0yHDA5XB","colab_type":"code","colab":{}},"source":["immagini_db_zeropadding = np.load(PATH_BASE + \"input_imgs_dataset_zeropadding.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wr8u_zaNf6Lt","colab_type":"text"},"source":["# Loading Clusterizzazione in 40 normali pixel per pixel"]},{"cell_type":"code","metadata":{"id":"2Eytvr3ogH4c","colab_type":"code","colab":{}},"source":["reshaped_labels = np.load(PATH_BASE + \"normals_centroid_labels.npy\")\n","codebook = np.load(PATH_BASE + \"codebook_labels_3d_components.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3fIUXHoAPeSO","colab_type":"text"},"source":["## Zero padding anche sulle label"]},{"cell_type":"code","metadata":{"id":"GQO_mtTg4uOs","colab_type":"code","colab":{}},"source":["reshaped_labels_zeropadding = np.load(PATH_BASE + \"normals_centroid_labels_zeropadding.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dkifSxZGPhN_","colab_type":"text"},"source":["## Passaggio a 41 channel"]},{"cell_type":"markdown","metadata":{"id":"urThIMc80uxG","colab_type":"text"},"source":["Si passa da WxH a WxHxC dove C rappresenta i singoli cluster"]},{"cell_type":"code","metadata":{"id":"yxLK-vw5_P__","colab_type":"code","colab":{}},"source":["reshaped_labels_processed_zeropadding = np.load(PATH_BASE + \"normals_centroid_labels_41channels.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0vrPAlXTgViQ","colab_type":"text"},"source":["# Modellizzazione"]},{"cell_type":"code","metadata":{"id":"XbeGcEY8h6SE","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","x_trainval, x_test, y_trainval, y_test = train_test_split(immagini_db_zeropadding, reshaped_labels_processed_zeropadding, test_size=0.3, random_state=1221)\n","x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.2, random_state=2442)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CcRpTOmTjfwQ","colab_type":"code","outputId":"03e87d40-22ce-4ad0-de43-4190aabe307b","executionInfo":{"status":"ok","timestamp":1588316359296,"user_tz":-120,"elapsed":5537,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import cv2\n","import tensorflow as tf\n","import keras\n","#import tensorflow.keras\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"31NF5OukjlNv","colab_type":"code","colab":{}},"source":["# helper function for data visualization\n","def visualize(**images):\n","    \"\"\"PLot images in one row.\"\"\"\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()\n","    \n","# helper function for data visualization    \n","def denormalize(x):\n","    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n","    x_max = np.percentile(x, 98)\n","    x_min = np.percentile(x, 2)    \n","    x = (x - x_min) / (x_max - x_min)\n","    x = x.clip(0, 1)\n","    return x\n","    \n","\n","# classes for data loading and preprocessing\n","class Dataset:\n","    \"\"\"Normal surface dataset. Read images, apply augmentation and preprocessing transformations.\n","    \n","    Args:\n","        x (nparray): images\n","        y (nparray): label\n","        augmentation (albumentations.Compose): data transfromation pipeline \n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing \n","            (e.g. noralization, shape manipulation, etc.)\n","    \n","    \"\"\"\n","    \n","    def __init__(\n","            self, \n","            x, \n","            y, \n","            augmentation=None, \n","            preprocessing=None,\n","    ):\n","        self.x = x\n","        self.y = y\n","        \n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","    \n","    def __getitem__(self, i):\n","        # read data\n","        image = self.x[i]\n","        label = self.y[i]\n","        \n","        # apply augmentations\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, label=label)\n","            image, mask = sample['image'], sample['label']\n","        \n","        # apply preprocessing\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, label=label)\n","            image, mask = sample['image'], sample['label']\n","            \n","        return image, label\n","        \n","    def __len__(self):\n","        return self.x.shape[0]\n","    \n","    \n","class Dataloder(keras.utils.Sequence):\n","    \"\"\"Load data from dataset and form batches\n","    \n","    Args:\n","        dataset: instance of Dataset class for image loading and preprocessing.\n","        batch_size: Integet number of images in batch.\n","        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n","    \"\"\"\n","    \n","    def __init__(self, dataset, batch_size=1, shuffle=False):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.indexes = np.arange(len(dataset))\n","\n","        self.on_epoch_end()\n","\n","    def __getitem__(self, i):\n","        \n","        # collect batch data\n","        start = i * self.batch_size\n","        stop = (i + 1) * self.batch_size\n","        data = []\n","        for j in range(start, stop):\n","            data.append(self.dataset[j])\n","        \n","        # transpose list of lists\n","        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n","        \n","        return batch\n","    \n","    def __len__(self):\n","        \"\"\"Denotes the number of batches per epoch\"\"\"\n","        return len(self.indexes) // self.batch_size\n","    \n","    def on_epoch_end(self):\n","        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n","        if self.shuffle:\n","            self.indexes = np.random.permutation(self.indexes)   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yyD_fERaLQp3","colab_type":"code","colab":{}},"source":["import albumentations as A\n","\n","def round_clip_0_1(x, **kwargs):\n","    return x.round().clip(0, 1)\n","\n","# define heavy augmentations\n","def get_training_augmentation():\n","    train_transform = [\n","\n","        A.HorizontalFlip(p=0.5),\n","\n","        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n","\n","        A.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n","        A.RandomCrop(height=320, width=320, always_apply=True),\n","\n","        A.IAAAdditiveGaussianNoise(p=0.2),\n","        A.IAAPerspective(p=0.5),\n","\n","        A.OneOf(\n","            [\n","                A.CLAHE(p=1),\n","                A.RandomBrightness(p=1),\n","                A.RandomGamma(p=1),\n","            ],\n","            p=0.9,\n","        ),\n","\n","        A.OneOf(\n","            [\n","                A.IAASharpen(p=1),\n","                A.Blur(blur_limit=3, p=1),\n","                A.MotionBlur(blur_limit=3, p=1),\n","            ],\n","            p=0.9,\n","        ),\n","\n","        A.OneOf(\n","            [\n","                A.RandomContrast(p=1),\n","                A.HueSaturationValue(p=1),\n","            ],\n","            p=0.9,\n","        ),\n","        A.Lambda(mask=round_clip_0_1)\n","    ]\n","    return A.Compose(train_transform)\n","\n","\n","def get_validation_augmentation():\n","    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n","    test_transform = [\n","        A.PadIfNeeded(384, 480)\n","    ]\n","    return A.Compose(test_transform)\n","\n","def get_preprocessing(preprocessing_fn):\n","    \"\"\"Construct preprocessing transform\n","    \n","    Args:\n","        preprocessing_fn (callbale): data normalization function \n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose\n","    \n","    \"\"\"\n","    \n","    _transform = [\n","        A.Lambda(image=preprocessing_fn),\n","    ]\n","    return A.Compose(_transform)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJeP8VH5kFzt","colab_type":"text"},"source":["Costruzione modello"]},{"cell_type":"code","metadata":{"id":"fvSmZ724kA38","colab_type":"code","outputId":"2a5864ad-8df7-46f0-8dcf-3b6da2c75f02","executionInfo":{"status":"ok","timestamp":1588324728811,"user_tz":-120,"elapsed":1625,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import segmentation_models as sm\n","\n","# segmentation_models could also use `tf.keras` if you do not have Keras installed\n","# or you could switch to other framework using `sm.set_framework('tf.keras')`"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Segmentation Models: using `keras` framework.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8fDpbHNpkJ5o","colab_type":"code","colab":{}},"source":["BACKBONE = 'efficientnetb3'\n","TRAIN_BATCH_SIZE = 8\n","VAL_BATCH_SIZE = 4\n","LR = 0.0001\n","EPOCHS = 40\n","\n","preprocess_input = sm.get_preprocessing(BACKBONE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdfO0YcOkKjz","colab_type":"code","colab":{}},"source":["# define network parameters\n","n_classes = 41\n","activation = 'softmax'\n","\n","#create model\n","model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gyH57h0w-eOC","colab_type":"text"},"source":["Calcolo dei pesi"]},{"cell_type":"code","metadata":{"id":"J2PmFmpn-x0Z","colab_type":"code","colab":{}},"source":["class_weight = np.load(PATH_BASE + \"label_weights_zeropadding.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuoV1EQVkw6b","colab_type":"code","colab":{}},"source":["# define optomizer\n","optim = keras.optimizers.Adam(LR)\n","\n","# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n","# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n","dice_loss = sm.losses.DiceLoss(class_weights=class_weight)\n","focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n","total_loss = dice_loss + (1 * focal_loss)\n","\n","# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n","# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n","\n","metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n","\n","# compile keras model with defined optimozer, loss and metrics\n","model.compile(optim, total_loss, metrics)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Hky3fONufbf","colab_type":"code","colab":{}},"source":["# Dataset for train images\n","train_dataset = Dataset(\n","    x_train, \n","    y_train\n","    #augmentation=get_training_augmentation(),\n","    #preprocessing=get_preprocessing(preprocess_input),\n",")\n","\n","# Dataset for validation images\n","valid_dataset = Dataset(\n","    x_val, \n","    y_val\n","    #augmentation=get_validation_augmentation(),\n","    #preprocessing=get_preprocessing(preprocess_input),\n",")\n","\n","train_dataloader = Dataloder(train_dataset, TRAIN_BATCH_SIZE, shuffle=True)\n","valid_dataloader = Dataloder(valid_dataset, VAL_BATCH_SIZE, shuffle=False)\n","\n","# check shapes for errors\n","assert train_dataloader[0][0].shape == (TRAIN_BATCH_SIZE, 320, 320, 3)\n","assert train_dataloader[0][1].shape == (TRAIN_BATCH_SIZE, 320, 320, 41)\n","\n","# define callbacks for learning rate scheduling and best checkpoints saving\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\n","    keras.callbacks.ReduceLROnPlateau(),\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0pkZ5oBgrQm7","colab_type":"code","outputId":"4562bbce-e07f-4c5e-cce5-aa23b2e947e4","executionInfo":{"status":"ok","timestamp":1588323433794,"user_tz":-120,"elapsed":6731055,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# train model\n","history = model.fit_generator(\n","    train_dataloader, \n","    steps_per_epoch=len(train_dataloader), \n","    epochs=EPOCHS, \n","    callbacks=callbacks, \n","    validation_data=valid_dataloader, \n","    validation_steps=len(valid_dataloader),\n",")"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Epoch 1/40\n","101/101 [==============================] - 172s 2s/step - loss: 0.9834 - iou_score: 1.2768e-04 - f1-score: 2.4113e-04 - val_loss: 0.9834 - val_iou_score: 0.0012 - val_f1-score: 0.0021\n","Epoch 2/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.9508 - iou_score: 0.0022 - f1-score: 0.0039 - val_loss: 0.9233 - val_iou_score: 0.0022 - val_f1-score: 0.0039\n","Epoch 3/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.8903 - iou_score: 0.0101 - f1-score: 0.0166 - val_loss: 0.8887 - val_iou_score: 0.0148 - val_f1-score: 0.0235\n","Epoch 4/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.8343 - iou_score: 0.0215 - f1-score: 0.0341 - val_loss: 0.8759 - val_iou_score: 0.0155 - val_f1-score: 0.0242\n","Epoch 5/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.7919 - iou_score: 0.0292 - f1-score: 0.0457 - val_loss: 0.8571 - val_iou_score: 0.0184 - val_f1-score: 0.0287\n","Epoch 6/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.7489 - iou_score: 0.0367 - f1-score: 0.0576 - val_loss: 0.7962 - val_iou_score: 0.0229 - val_f1-score: 0.0369\n","Epoch 7/40\n","101/101 [==============================] - 167s 2s/step - loss: 0.6952 - iou_score: 0.0478 - f1-score: 0.0761 - val_loss: 0.7680 - val_iou_score: 0.0262 - val_f1-score: 0.0429\n","Epoch 8/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.6459 - iou_score: 0.0754 - f1-score: 0.1100 - val_loss: 0.7350 - val_iou_score: 0.0346 - val_f1-score: 0.0571\n","Epoch 9/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.5939 - iou_score: 0.0898 - f1-score: 0.1274 - val_loss: 0.7074 - val_iou_score: 0.0594 - val_f1-score: 0.0820\n","Epoch 10/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.5479 - iou_score: 0.0953 - f1-score: 0.1358 - val_loss: 0.7267 - val_iou_score: 0.0587 - val_f1-score: 0.0812\n","Epoch 11/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.5054 - iou_score: 0.1004 - f1-score: 0.1434 - val_loss: 0.6830 - val_iou_score: 0.0623 - val_f1-score: 0.0871\n","Epoch 12/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.4656 - iou_score: 0.1057 - f1-score: 0.1513 - val_loss: 0.6869 - val_iou_score: 0.0630 - val_f1-score: 0.0881\n","Epoch 13/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.4296 - iou_score: 0.1098 - f1-score: 0.1577 - val_loss: 0.6647 - val_iou_score: 0.0623 - val_f1-score: 0.0868\n","Epoch 14/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.4020 - iou_score: 0.1134 - f1-score: 0.1633 - val_loss: 0.6940 - val_iou_score: 0.0642 - val_f1-score: 0.0902\n","Epoch 15/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.3661 - iou_score: 0.1191 - f1-score: 0.1714 - val_loss: 0.6370 - val_iou_score: 0.0664 - val_f1-score: 0.0944\n","Epoch 16/40\n","101/101 [==============================] - 167s 2s/step - loss: 0.3347 - iou_score: 0.1256 - f1-score: 0.1822 - val_loss: 0.6251 - val_iou_score: 0.0664 - val_f1-score: 0.0943\n","Epoch 17/40\n","101/101 [==============================] - 167s 2s/step - loss: 0.3046 - iou_score: 0.1313 - f1-score: 0.1907 - val_loss: 0.5992 - val_iou_score: 0.0710 - val_f1-score: 0.1024\n","Epoch 18/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.2740 - iou_score: 0.1372 - f1-score: 0.1994 - val_loss: 0.5753 - val_iou_score: 0.0713 - val_f1-score: 0.1023\n","Epoch 19/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.2438 - iou_score: 0.1433 - f1-score: 0.2078 - val_loss: 0.5709 - val_iou_score: 0.0737 - val_f1-score: 0.1061\n","Epoch 20/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.2175 - iou_score: 0.1479 - f1-score: 0.2143 - val_loss: 0.5678 - val_iou_score: 0.0730 - val_f1-score: 0.1055\n","Epoch 21/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.1892 - iou_score: 0.1541 - f1-score: 0.2226 - val_loss: 0.5485 - val_iou_score: 0.0775 - val_f1-score: 0.1129\n","Epoch 22/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.1713 - iou_score: 0.1584 - f1-score: 0.2291 - val_loss: 0.5682 - val_iou_score: 0.0782 - val_f1-score: 0.1141\n","Epoch 23/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.1493 - iou_score: 0.1638 - f1-score: 0.2368 - val_loss: 0.5775 - val_iou_score: 0.0760 - val_f1-score: 0.1107\n","Epoch 24/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.1296 - iou_score: 0.1683 - f1-score: 0.2433 - val_loss: 0.5718 - val_iou_score: 0.0794 - val_f1-score: 0.1161\n","Epoch 25/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.1091 - iou_score: 0.1735 - f1-score: 0.2505 - val_loss: 0.5649 - val_iou_score: 0.0812 - val_f1-score: 0.1189\n","Epoch 26/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.0925 - iou_score: 0.1777 - f1-score: 0.2564 - val_loss: 0.5584 - val_iou_score: 0.0808 - val_f1-score: 0.1184\n","Epoch 27/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.0737 - iou_score: 0.1830 - f1-score: 0.2638 - val_loss: 0.5597 - val_iou_score: 0.0825 - val_f1-score: 0.1215\n","Epoch 28/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.0547 - iou_score: 0.1880 - f1-score: 0.2710 - val_loss: 0.5231 - val_iou_score: 0.0856 - val_f1-score: 0.1267\n","Epoch 29/40\n","101/101 [==============================] - 168s 2s/step - loss: 0.0359 - iou_score: 0.1944 - f1-score: 0.2805 - val_loss: 0.5329 - val_iou_score: 0.0868 - val_f1-score: 0.1288\n","Epoch 30/40\n","101/101 [==============================] - 167s 2s/step - loss: 0.0207 - iou_score: 0.1993 - f1-score: 0.2885 - val_loss: 0.5216 - val_iou_score: 0.0883 - val_f1-score: 0.1312\n","Epoch 31/40\n","101/101 [==============================] - 167s 2s/step - loss: 0.0061 - iou_score: 0.2072 - f1-score: 0.3010 - val_loss: 0.5318 - val_iou_score: 0.0879 - val_f1-score: 0.1308\n","Epoch 32/40\n","101/101 [==============================] - 167s 2s/step - loss: -0.0140 - iou_score: 0.2153 - f1-score: 0.3132 - val_loss: 0.4837 - val_iou_score: 0.0917 - val_f1-score: 0.1374\n","Epoch 33/40\n","101/101 [==============================] - 168s 2s/step - loss: -0.0332 - iou_score: 0.2235 - f1-score: 0.3253 - val_loss: 0.5310 - val_iou_score: 0.0928 - val_f1-score: 0.1393\n","Epoch 34/40\n","101/101 [==============================] - 168s 2s/step - loss: -0.0511 - iou_score: 0.2313 - f1-score: 0.3369 - val_loss: 0.4767 - val_iou_score: 0.0919 - val_f1-score: 0.1382\n","Epoch 35/40\n","101/101 [==============================] - 168s 2s/step - loss: -0.0699 - iou_score: 0.2391 - f1-score: 0.3484 - val_loss: 0.4738 - val_iou_score: 0.0949 - val_f1-score: 0.1434\n","Epoch 36/40\n","101/101 [==============================] - 168s 2s/step - loss: -0.0839 - iou_score: 0.2443 - f1-score: 0.3560 - val_loss: 0.4775 - val_iou_score: 0.0978 - val_f1-score: 0.1477\n","Epoch 37/40\n","101/101 [==============================] - 167s 2s/step - loss: -0.0995 - iou_score: 0.2503 - f1-score: 0.3643 - val_loss: 0.5212 - val_iou_score: 0.0976 - val_f1-score: 0.1478\n","Epoch 38/40\n","101/101 [==============================] - 168s 2s/step - loss: -0.1140 - iou_score: 0.2564 - f1-score: 0.3727 - val_loss: 0.5371 - val_iou_score: 0.0989 - val_f1-score: 0.1498\n","Epoch 39/40\n","101/101 [==============================] - 168s 2s/step - loss: -0.1286 - iou_score: 0.2612 - f1-score: 0.3791 - val_loss: 0.5168 - val_iou_score: 0.1003 - val_f1-score: 0.1518\n","Epoch 40/40\n","101/101 [==============================] - 168s 2s/step - loss: -0.1438 - iou_score: 0.2661 - f1-score: 0.3860 - val_loss: 0.4996 - val_iou_score: 0.0993 - val_f1-score: 0.1502\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ez6sLWAggXI2","colab_type":"text"},"source":["# Ricalcolo superfici normali da classificazione"]},{"cell_type":"code","metadata":{"id":"3gshBXstgX3Y","colab_type":"code","colab":{}},"source":["# Metodo che permette di ottenere la normale 3d per ogni pixel a partire dalla predizione della rete.\n","# INPUT\n","# netPrediction: shape = (H, W, 40). Predizione della rete di un'immagine. \n","#                Per ogni pixel la rete fornisce un vettore di 40 probabilità (1 per ogni centroide).\n","# codebook:      shape = (40,3). Lista delle 40 normali di riferimento (centroidi). codebook.shape = (40,3)\n","# triangoli:     shape = (N, 3). Triangolazione di delaunay; ogni elemento di questo vettore è una tripla di indici di centroidi, da utilizzare\n","#                nel codebook ottenuto con la clusterizzazione. (es. N = 67)\n","# OUTPUT\n","# norm:          shape = (H, W, 3). Normali 3d pixel per pixel dell'immagine.\n","\n","def decode(netPrediction, codebook, triangoli):\n","  h, w = netPrediction.shape[0:2]\n","  \n","  # for each triangle, get total prob\n","  tri_prob = np.dstack([np.sum(netPrediction[:,:,t], axis = -1) for t in triangoli]) \n","\n","  # get best tri\n","  best_tri = np.argmax(tri_prob, axis = -1)\n","  \n","  # get coefficients to most probable tri\n","  alphas = np.reshape([netPrediction[row, col, triangoli[best_tri[row, col]]] for row in range(h) for col in range(w)], (h, w, 3))\n","\n","  # sum -> 1\n","  alphas = np.divide(alphas, np.reshape(np.tile(np.sum(alphas, axis = -1), (1,3)), (h, w, 3)))\n","  alphas = np.reshape(np.tile(alphas, (1,1,3)), (h, w,3,3))  \n","  alphas = np.swapaxes(alphas, 2,3)\n","\n","  norm = np.multiply(alphas, codebook[triangoli[best_tri]])\n","  norm = np.sum(norm, axis = -2)\n","  \n","  return norm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"thq_gXo1gDDX","colab_type":"code","colab":{}},"source":["from keras.models import load_model\n","model.load_weights(\"best_model.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4RJgsSxiUge","colab_type":"code","colab":{}},"source":["y_test_p = model.predict_cla(x_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3pQS4nYKgzK1","colab_type":"code","colab":{}},"source":["# chiamo la funzione di decodifica e ottengo la mappa delle normali 3d\n","netPrediction_3d = decode(model.predict(x_test), codebook, triangoli)\n","print(netPrediction_3d.shape)\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","#plt.figure('es. predizione decodificata')\n","plt.imshow(255/2 * netPrediction_3d)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWaT7KxAhNTD","colab_type":"text"},"source":["# Valutazione"]},{"cell_type":"code","metadata":{"id":"XAhRq-5NhbO5","colab_type":"code","colab":{}},"source":["# pixel wise evaluation\n","# INPUT\n","# prediction:   shape = (1,3) normale predetta dalla rete (ottenuta come codifica dalle label)\n","# ground_truth: shape = (1,3) normale effettiva\n","# OUTPUT\n","# l'errore in radianti tra la normale passata e quella effettiva\n","def eval_cosine(prediction, ground_truth):\n","  if np.sum(ground_truth) == 0 or np.sum(prediction) == 0:\n","    return 0\n","  cosine = np.dot(prediction, ground_truth) / (np.linalg.norm(prediction) * np.linalg.norm(ground_truth))\n","  if (cosine <= 1 and cosine >= -1):\n","    return np.arccos(cosine) \n","  return np.deg2rad(180) if cosine < -1 else 0\n","\n","# questa funzione va chiamata su tutti i pixel di tutte le immagini del test-set o validation set."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DffV2pi-hdJj","colab_type":"code","colab":{}},"source":["# TODO: effettuare la predizione su tutto il test-set (es: 30% del dataset \"normals_orig.npy\") con la rete. \n","# Dopodichè decodificare le predizioni per ogni immagine (label -> normali 3d) e possibilmente salvare in un file (es: \"prediction_test_decoded.npy\").\n","\n","# es. di test-set\n","idx_test = np.random.randint(0, N, int(0.3 * N))\n","normali_test = normali[idx_test]\n","\n","# NOTA: nel calcolo della metrica non vanno considerati i pixel rumorosi, che sono identificati dalla tripla [0,0,0].\n","tot_noised_pixel = np.where(np.sum(normali_test, axis = -1) == 0)[0].shape[0]\n","tot_pixel = normali_test.shape[0] * normali_test.shape[1] * normali_test.shape[2]\n","valid_pixel = tot_pixel - tot_noised_pixel\n","\n","print('Noised pixel / Tot pixel (%):', tot_noised_pixel / tot_pixel * 100)\n","print('Mean: ', np.rad2deg(np.sum(theta) / valid_pixel))\n","\n","# es: carico da file\n","# theta = np.load(\"prediction_test_decoded.npy\")\n","\n","# valuto la percentuale di errori angolari inferiori rispetto ad una delle seguenti soglie\n","soglie_errori = [11.25, 22.5, 30]\n","for th in soglie_errori:\n","  under_th = (np.where(theta < np.deg2rad(th))[0].shape[0] - noised_pixel) / valid_pixel * 100\n","  print('Threshold', str(th),  under_th)"],"execution_count":0,"outputs":[]}]}