{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"normal_estimation_classification.ipynb","provenance":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNcjHDPCx7JCegfKVigXofM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JVK2YhY_fQXT","colab_type":"text"},"source":["# Connessione a directory Drive"]},{"cell_type":"code","metadata":{"id":"i6Nt5L3oermW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6169caf5-a63d-4c2b-b87c-363d75f4c0b4","executionInfo":{"status":"ok","timestamp":1587896313986,"user_tz":-120,"elapsed":523,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JKAzI_xNfUWB","colab_type":"text"},"source":["# Loading Dataset immagini di input"]},{"cell_type":"code","metadata":{"id":"THTHHUIVfPUP","colab_type":"code","colab":{}},"source":["import numpy as np\n","PATH_BASE = '/content/drive/My Drive/Appunti delle lezioni/2Anno2Semestre/Digital Image Processing/surface_normal_estimation/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gE2iJUt0faRn","colab_type":"code","colab":{}},"source":["immagini_db = np.load(PATH_BASE + 'input_imgs_dataset.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"guc-sX0Nfka-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d8fbb288-b51e-428b-b7cd-dbfb5a5faf8a","executionInfo":{"status":"ok","timestamp":1587896333764,"user_tz":-120,"elapsed":642,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["print(immagini_db.shape)\n","N = immagini_db.shape[0]"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(1446, 195, 260, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wr8u_zaNf6Lt","colab_type":"text"},"source":["# Loading Clusterizzazione in 40 normali pixel per pixel"]},{"cell_type":"code","metadata":{"id":"2Eytvr3ogH4c","colab_type":"code","colab":{}},"source":["reshaped_labels = np.load(PATH_BASE + \"normals_centroid_labels.npy\")\n","codebook = np.load(PATH_BASE + \"codebook_labels_3d_components.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IJYl1XRy1AFQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"25864a41-e9c2-4cf7-8c45-c39476b5e351","executionInfo":{"status":"ok","timestamp":1587896352091,"user_tz":-120,"elapsed":575,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["reshaped_labels.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1446, 195, 260)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"urThIMc80uxG","colab_type":"text"},"source":["Si passa da WxH a WxHxC dove C rappresenta i singoli cluster"]},{"cell_type":"code","metadata":{"id":"7Oz6DNaI06gX","colab_type":"code","colab":{}},"source":["def process_channels(reshaped_labels):\n","  reshaped_labels_processed = np.zeros((reshaped_labels.shape[0], reshaped_labels.shape[1], reshaped_labels.shape[2], 40), dtype=\"uint8\")\n","  for n in range(0, reshaped_labels_processed.shape[0]):\n","    for i in range(0, reshaped_labels_processed.shape[1]):\n","      for j in range(0, reshaped_labels_processed.shape[2]):\n","        reshaped_labels_processed[n][i][j][reshaped_labels[n][i][j]] = 1\n","  return reshaped_labels_processed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfnB1bQy8lCO","colab_type":"code","colab":{}},"source":["np.save('/content/drive/My Drive/Appunti delle lezioni/2Anno2Semestre/Digital Image Processing/' + \"normals_centroid_labels_40channels.npy\", reshaped_labels_processed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxLK-vw5_P__","colab_type":"code","colab":{}},"source":["reshaped_labels_processed = np.load('/content/drive/My Drive/Appunti delle lezioni/2Anno2Semestre/Digital Image Processing/normals_centroid_labels_40channels.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0vrPAlXTgViQ","colab_type":"text"},"source":["# Modellizzazione"]},{"cell_type":"code","metadata":{"id":"XbeGcEY8h6SE","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","x_trainval, x_test, y_trainval, y_test = train_test_split(immagini_db, reshaped_labels_processed, test_size=0.3, random_state=1221)\n","x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.2, random_state=2442)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0jTWjq3itoD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":658},"outputId":"2212b91a-4e5f-4319-a9ce-08dc6b2c999d","executionInfo":{"status":"ok","timestamp":1587896289947,"user_tz":-120,"elapsed":9140,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["# Install required libs\n","\n","### please update Albumentations to version>=0.3.0 for `Lambda` transform support\n","!pip install -U albumentations==0.3.0 --user \n","!pip install -U --pre segmentation-models --user"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: albumentations==0.3.0 in /root/.local/lib/python3.6/site-packages (0.3.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.0) (1.18.3)\n","Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.0) (3.13)\n","Requirement already satisfied, skipping upgrade: opencv-python-headless in /root/.local/lib/python3.6/site-packages (from albumentations==0.3.0) (4.2.0.34)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.0) (1.4.1)\n","Requirement already satisfied, skipping upgrade: imgaug<0.2.7,>=0.2.5 in /root/.local/lib/python3.6/site-packages (from albumentations==0.3.0) (0.2.6)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (1.12.0)\n","Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (0.16.2)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (3.2.1)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (1.1.1)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.4.1)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.4)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (7.0.0)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (0.10.0)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.4.7)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (2.8.1)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (1.2.0)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.0) (4.4.2)\n","Requirement already up-to-date: segmentation-models in /root/.local/lib/python3.6/site-packages (1.0.1)\n","Requirement already satisfied, skipping upgrade: image-classifiers==1.0.0 in /root/.local/lib/python3.6/site-packages (from segmentation-models) (1.0.0)\n","Requirement already satisfied, skipping upgrade: efficientnet==1.0.0 in /root/.local/lib/python3.6/site-packages (from segmentation-models) (1.0.0)\n","Requirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.8)\n","Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.16.2)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (2.10.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.18.3)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.1.1)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.1)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.0.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.12.0)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models) (4.4.2)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.7)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.1)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.2.0)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.10.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CcRpTOmTjfwQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"697a5837-7bb5-4f57-e33c-4f61ff7a81cb","executionInfo":{"status":"ok","timestamp":1587896451258,"user_tz":-120,"elapsed":2846,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["import cv2\n","import keras\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"31NF5OukjlNv","colab_type":"code","colab":{}},"source":["# helper function for data visualization\n","def visualize(**images):\n","    \"\"\"PLot images in one row.\"\"\"\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()\n","    \n","# helper function for data visualization    \n","def denormalize(x):\n","    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n","    x_max = np.percentile(x, 98)\n","    x_min = np.percentile(x, 2)    \n","    x = (x - x_min) / (x_max - x_min)\n","    x = x.clip(0, 1)\n","    return x\n","    \n","\n","# classes for data loading and preprocessing\n","class Dataset:\n","    \"\"\"Normal surface dataset. Read images, apply augmentation and preprocessing transformations.\n","    \n","    Args:\n","        x (nparray): images\n","        y (nparray): label\n","        augmentation (albumentations.Compose): data transfromation pipeline \n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing \n","            (e.g. noralization, shape manipulation, etc.)\n","    \n","    \"\"\"\n","    \n","    def __init__(\n","            self, \n","            x, \n","            y, \n","            augmentation=None, \n","            preprocessing=None,\n","    ):\n","        self.x = x\n","        self.y = y\n","        \n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","    \n","    def __getitem__(self, i):\n","        # read data\n","        image = self.x[i]\n","        label = self.y[i]\n","        \n","        # apply augmentations\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","        \n","        # apply preprocessing\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","            \n","        return image, label\n","        \n","    def __len__(self):\n","        return self.x.shape[0]\n","    \n","    \n","class Dataloder(keras.utils.Sequence):\n","    \"\"\"Load data from dataset and form batches\n","    \n","    Args:\n","        dataset: instance of Dataset class for image loading and preprocessing.\n","        batch_size: Integet number of images in batch.\n","        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n","    \"\"\"\n","    \n","    def __init__(self, dataset, batch_size=1, shuffle=False):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.indexes = np.arange(len(dataset))\n","\n","        self.on_epoch_end()\n","\n","    def __getitem__(self, i):\n","        \n","        # collect batch data\n","        start = i * self.batch_size\n","        stop = (i + 1) * self.batch_size\n","        data = []\n","        for j in range(start, stop):\n","            data.append(self.dataset[j])\n","        \n","        # transpose list of lists\n","        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n","        \n","        return batch\n","    \n","    def __len__(self):\n","        \"\"\"Denotes the number of batches per epoch\"\"\"\n","        return len(self.indexes) // self.batch_size\n","    \n","    def on_epoch_end(self):\n","        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n","        if self.shuffle:\n","            self.indexes = np.random.permutation(self.indexes)   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJeP8VH5kFzt","colab_type":"text"},"source":["Costruzione modello"]},{"cell_type":"code","metadata":{"id":"fvSmZ724kA38","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"338210e7-2bb9-4dac-abe9-ff9785af171a","executionInfo":{"status":"ok","timestamp":1587896461282,"user_tz":-120,"elapsed":806,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["import segmentation_models as sm\n","\n","# segmentation_models could also use `tf.keras` if you do not have Keras installed\n","# or you could switch to other framework using `sm.set_framework('tf.keras')`"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Segmentation Models: using `keras` framework.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8fDpbHNpkJ5o","colab_type":"code","colab":{}},"source":["BACKBONE = 'efficientnetb3'\n","BATCH_SIZE = 8\n","LR = 0.0001\n","EPOCHS = 40\n","\n","preprocess_input = sm.get_preprocessing(BACKBONE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdfO0YcOkKjz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"b5b1d340-b3c6-4dfd-de05-8c112991a43f","executionInfo":{"status":"ok","timestamp":1587896482062,"user_tz":-120,"elapsed":15317,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["# define network parameters\n","n_classes = 40\n","activation = 'softmax'\n","\n","#create model\n","model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n","44113920/44107200 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cd13xtLxkYHb","colab_type":"code","colab":{}},"source":["masksOccurency = np.arange(40)\n","for n in range(0, reshaped_labels.shape[0]):\n","  for i in range(0, reshaped_labels.shape[1]):\n","    for j in range(0, reshaped_labels.shape[2]):\n","      masksOccurency[reshaped_labels[n][i][j]] += 1\n","\n","labelList = []\n","for k in range(0, len(masksOccurency)):\n","  labelList.extend([k] * masksOccurency[k])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptivEFptkuVd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151},"outputId":"6daff7c4-65a9-4bf5-8c37-a34550caedd7","executionInfo":{"status":"ok","timestamp":1587896589283,"user_tz":-120,"elapsed":15900,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["from sklearn.utils import class_weight\n","class_weights = class_weight.compute_class_weight('balanced',np.unique(labelList),labelList)\n","#class_weights = dict(enumerate(class_weights)) dictionary for keras\n","class_weights"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2.1202915 , 0.52198782, 1.26013135, 2.23997597, 0.8550544 ,\n","       2.71960664, 0.61911924, 0.62490245, 0.90564414, 3.89039725,\n","       0.67618402, 1.58404131, 0.95352983, 0.27496577, 0.64053463,\n","       4.7687085 , 3.97269029, 2.47241303, 0.9518679 , 4.28935562,\n","       0.79128958, 2.13852942, 1.88429593, 2.50496736, 0.65771088,\n","       1.36489588, 4.78414347, 1.34214255, 0.83879863, 5.30564513,\n","       3.18227427, 0.91597808, 3.68038526, 0.90965939, 0.67399732,\n","       0.88036779, 0.78057669, 1.77479062, 0.21835131, 1.46587261])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"yuoV1EQVkw6b","colab_type":"code","colab":{}},"source":["# define optomizer\n","optim = keras.optimizers.Adam(LR)\n","\n","# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n","# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n","dice_loss = sm.losses.DiceLoss(class_weights=class_weights.tolist())\n","focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n","total_loss = dice_loss + (1 * focal_loss)\n","\n","# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n","# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n","\n","metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n","\n","# compile keras model with defined optimozer, loss and metrics\n","model.compile(optim, total_loss, metrics)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Hky3fONufbf","colab_type":"code","colab":{}},"source":["# Dataset for train images\n","train_dataset = Dataset(\n","    x_train, \n","    y_train \n","    #augmentation=get_training_augmentation(),\n","    #preprocessing=get_preprocessing(preprocess_input),\n",")\n","\n","# Dataset for validation images\n","valid_dataset = Dataset(\n","    x_val, \n","    y_val\n","    #augmentation=get_validation_augmentation(),\n","    #preprocessing=get_preprocessing(preprocess_input),\n",")\n","\n","train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)\n","\n","# check shapes for errors\n","assert train_dataloader[0][0].shape == (BATCH_SIZE, 195, 260, 3)\n","assert train_dataloader[0][1].shape == (BATCH_SIZE, 195, 260, 40)\n","\n","# define callbacks for learning rate scheduling and best checkpoints saving\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\n","    keras.callbacks.ReduceLROnPlateau(),\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Dnd1FuuCQpg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d579ac38-e157-4649-95ef-4358edf765ff","executionInfo":{"status":"ok","timestamp":1587896987200,"user_tz":-120,"elapsed":618,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["len(train_dataset)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["809"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"UqJP11ZnCTi_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"06b1201a-20d7-4de9-c5c7-491aa85d0e07","executionInfo":{"status":"ok","timestamp":1587897001050,"user_tz":-120,"elapsed":593,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["len(valid_dataset)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["203"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"0pkZ5oBgrQm7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":458},"outputId":"82e077bd-201c-4cda-d973-75bb9f6f8499","executionInfo":{"status":"error","timestamp":1587896680379,"user_tz":-120,"elapsed":70983,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}}},"source":["# train model\n","history = model.fit_generator(\n","    train_dataloader, \n","    steps_per_epoch=len(train_dataloader), \n","    epochs=EPOCHS, \n","    callbacks=callbacks, \n","    validation_data=valid_dataloader, \n","    validation_steps=len(valid_dataloader),\n",")"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Epoch 1/40\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-fc2eae185d4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m:  ConcatOp : Dimensions of inputs should match: shape[0] = [8,1536,14,18] vs. shape[1] = [8,816,13,17]\n\t [[node decoder_stage0_concat/concat (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_81766]\n\nFunction call stack:\nkeras_scratch_graph\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ez6sLWAggXI2","colab_type":"text"},"source":["# Ricalcolo superfici normali da classificazione"]},{"cell_type":"code","metadata":{"id":"3gshBXstgX3Y","colab_type":"code","colab":{}},"source":["# Metodo che permette di ottenere la normale 3d per ogni pixel a partire dalla predizione della rete.\n","# INPUT\n","# netPrediction: shape = (H, W, 40). Predizione della rete di un'immagine. \n","#                Per ogni pixel la rete fornisce un vettore di 40 probabilità (1 per ogni centroide).\n","# codebook:      shape = (40,3). Lista delle 40 normali di riferimento (centroidi). codebook.shape = (40,3)\n","# triangoli:     shape = (N, 3). Triangolazione di delaunay; ogni elemento di questo vettore è una tripla di indici di centroidi, da utilizzare\n","#                nel codebook ottenuto con la clusterizzazione. (es. N = 67)\n","# OUTPUT\n","# norm:          shape = (H, W, 3). Normali 3d pixel per pixel dell'immagine.\n","\n","def decode(netPrediction, codebook, triangoli):\n","  h, w = netPrediction.shape[0:2]\n","  \n","  # for each triangle, get total prob\n","  tri_prob = np.dstack([np.sum(netPrediction[:,:,t], axis = -1) for t in triangoli]) \n","\n","  # get best tri\n","  best_tri = np.argmax(tri_prob, axis = -1)\n","  \n","  # get coefficients to most probable tri\n","  alphas = np.reshape([netPrediction[row, col, triangoli[best_tri[row, col]]] for row in range(h) for col in range(w)], (h, w, 3))\n","\n","  # sum -> 1\n","  alphas = np.divide(alphas, np.reshape(np.tile(np.sum(alphas, axis = -1), (1,3)), (h, w, 3)))\n","  alphas = np.reshape(np.tile(alphas, (1,1,3)), (h, w,3,3))  \n","  alphas = np.swapaxes(alphas, 2,3)\n","\n","  norm = np.multiply(alphas, codebook[triangoli[best_tri]])\n","  norm = np.sum(norm, axis = -2)\n","  \n","  return norm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3pQS4nYKgzK1","colab_type":"code","colab":{}},"source":["# chiamo la funzione di decodifica e ottengo la mappa delle normali 3d\n","netPrediction_3d = decode(netPrediction_mock, codebook, triangoli)\n","print(netPrediction_3d.shape)\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","#plt.figure('es. predizione decodificata')\n","plt.imshow(255/2 * netPrediction_3d)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWaT7KxAhNTD","colab_type":"text"},"source":["# Valutazione"]},{"cell_type":"code","metadata":{"id":"XAhRq-5NhbO5","colab_type":"code","colab":{}},"source":["# pixel wise evaluation\n","# INPUT\n","# prediction:   shape = (1,3) normale predetta dalla rete (ottenuta come codifica dalle label)\n","# ground_truth: shape = (1,3) normale effettiva\n","# OUTPUT\n","# l'errore in radianti tra la normale passata e quella effettiva\n","def eval_cosine(prediction, ground_truth):\n","  if np.sum(ground_truth) == 0 or np.sum(prediction) == 0:\n","    return 0\n","  cosine = np.dot(prediction, ground_truth) / (np.linalg.norm(prediction) * np.linalg.norm(ground_truth))\n","  if (cosine <= 1 and cosine >= -1):\n","    return np.arccos(cosine) \n","  return np.deg2rad(180) if cosine < -1 else 0\n","\n","# questa funzione va chiamata su tutti i pixel di tutte le immagini del test-set o validation set."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DffV2pi-hdJj","colab_type":"code","colab":{}},"source":["# TODO: effettuare la predizione su tutto il test-set (es: 30% del dataset \"normals_orig.npy\") con la rete. \n","# Dopodichè decodificare le predizioni per ogni immagine (label -> normali 3d) e possibilmente salvare in un file (es: \"prediction_test_decoded.npy\").\n","\n","# es. di test-set\n","idx_test = np.random.randint(0, N, int(0.3 * N))\n","normali_test = normali[idx_test]\n","\n","# NOTA: nel calcolo della metrica non vanno considerati i pixel rumorosi, che sono identificati dalla tripla [0,0,0].\n","tot_noised_pixel = np.where(np.sum(normali_test, axis = -1) == 0)[0].shape[0]\n","tot_pixel = normali_test.shape[0] * normali_test.shape[1] * normali_test.shape[2]\n","valid_pixel = tot_pixel - tot_noised_pixel\n","\n","print('Noised pixel / Tot pixel (%):', tot_noised_pixel / tot_pixel * 100)\n","print('Mean: ', np.rad2deg(np.sum(theta) / valid_pixel))\n","\n","# es: carico da file\n","# theta = np.load(\"prediction_test_decoded.npy\")\n","\n","# valuto la percentuale di errori angolari inferiori rispetto ad una delle seguenti soglie\n","soglie_errori = [11.25, 22.5, 30]\n","for th in soglie_errori:\n","  under_th = (np.where(theta < np.deg2rad(th))[0].shape[0] - noised_pixel) / valid_pixel * 100\n","  print('Threshold', str(th),  under_th)"],"execution_count":0,"outputs":[]}]}