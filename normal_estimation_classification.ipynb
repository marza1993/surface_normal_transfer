{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"normal_estimation_classification.ipynb","provenance":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"cXl2LIsHBk9A","colab_type":"text"},"source":["#Installazione pacchetti necessari"]},{"cell_type":"code","metadata":{"id":"NYFZ_SV6Bw30","colab_type":"code","colab":{}},"source":["# Install required libs\n","\n","### please update Albumentations to version>=0.3.0 for `Lambda` transform support\n","!pip install -U albumentations==0.3.0 --user \n","!pip install -U --pre segmentation-models --user"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JVK2YhY_fQXT","colab_type":"text"},"source":["# Connessione a directory Drive"]},{"cell_type":"code","metadata":{"id":"i6Nt5L3oermW","colab_type":"code","outputId":"180e3260-0bdb-4762-db83-8d97dd7a8662","executionInfo":{"status":"ok","timestamp":1587913724910,"user_tz":-120,"elapsed":715,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JKAzI_xNfUWB","colab_type":"text"},"source":["# Loading Dataset immagini di input"]},{"cell_type":"code","metadata":{"id":"THTHHUIVfPUP","colab_type":"code","colab":{}},"source":["import numpy as np\n","PATH_BASE = '/content/drive/My Drive/Appunti delle lezioni/2Anno2Semestre/Digital Image Processing/surface_normal_estimation/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gE2iJUt0faRn","colab_type":"code","colab":{}},"source":["immagini_db = np.load(PATH_BASE + 'input_imgs_dataset.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"guc-sX0Nfka-","colab_type":"code","outputId":"e0ef6ff2-e08f-4d46-fce2-fbff0403272e","executionInfo":{"status":"ok","timestamp":1587913734717,"user_tz":-120,"elapsed":3272,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(immagini_db.shape)\n","N = immagini_db.shape[0]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1446, 195, 260, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wr8u_zaNf6Lt","colab_type":"text"},"source":["# Loading Clusterizzazione in 40 normali pixel per pixel"]},{"cell_type":"code","metadata":{"id":"2Eytvr3ogH4c","colab_type":"code","colab":{}},"source":["reshaped_labels = np.load(PATH_BASE + \"normals_centroid_labels.npy\")\n","codebook = np.load(PATH_BASE + \"codebook_labels_3d_components.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IJYl1XRy1AFQ","colab_type":"code","outputId":"a4afac20-cf36-44f9-aa71-2d21e10843d8","executionInfo":{"status":"ok","timestamp":1587913741689,"user_tz":-120,"elapsed":712,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["reshaped_labels.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1446, 195, 260)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"urThIMc80uxG","colab_type":"text"},"source":["Si passa da WxH a WxHxC dove C rappresenta i singoli cluster"]},{"cell_type":"code","metadata":{"id":"7Oz6DNaI06gX","colab_type":"code","colab":{}},"source":["def process_channels(reshaped_labels):\n","  reshaped_labels_processed = np.zeros((reshaped_labels.shape[0], reshaped_labels.shape[1], reshaped_labels.shape[2], 40), dtype=\"uint8\")\n","  for n in range(0, reshaped_labels_processed.shape[0]):\n","    for i in range(0, reshaped_labels_processed.shape[1]):\n","      for j in range(0, reshaped_labels_processed.shape[2]):\n","        reshaped_labels_processed[n][i][j][reshaped_labels[n][i][j]] = 1\n","  return reshaped_labels_processed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfnB1bQy8lCO","colab_type":"code","colab":{}},"source":["np.save('/content/drive/My Drive/Appunti delle lezioni/2Anno2Semestre/Digital Image Processing/' + \"normals_centroid_labels_40channels.npy\", reshaped_labels_processed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxLK-vw5_P__","colab_type":"code","colab":{}},"source":["reshaped_labels_processed = np.load('/content/drive/My Drive/Appunti delle lezioni/2Anno2Semestre/Digital Image Processing/normals_centroid_labels_40channels.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0vrPAlXTgViQ","colab_type":"text"},"source":["# Modellizzazione"]},{"cell_type":"code","metadata":{"id":"XbeGcEY8h6SE","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","x_trainval, x_test, y_trainval, y_test = train_test_split(immagini_db, reshaped_labels_processed, test_size=0.3, random_state=1221)\n","x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.2, random_state=2442)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CcRpTOmTjfwQ","colab_type":"code","outputId":"64781eec-640b-4123-dc8b-c62d02cf7c91","executionInfo":{"status":"ok","timestamp":1587913806390,"user_tz":-120,"elapsed":2244,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import cv2\n","import keras\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"31NF5OukjlNv","colab_type":"code","colab":{}},"source":["# helper function for data visualization\n","def visualize(**images):\n","    \"\"\"PLot images in one row.\"\"\"\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()\n","    \n","# helper function for data visualization    \n","def denormalize(x):\n","    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n","    x_max = np.percentile(x, 98)\n","    x_min = np.percentile(x, 2)    \n","    x = (x - x_min) / (x_max - x_min)\n","    x = x.clip(0, 1)\n","    return x\n","    \n","\n","# classes for data loading and preprocessing\n","class Dataset:\n","    \"\"\"Normal surface dataset. Read images, apply augmentation and preprocessing transformations.\n","    \n","    Args:\n","        x (nparray): images\n","        y (nparray): label\n","        augmentation (albumentations.Compose): data transfromation pipeline \n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing \n","            (e.g. noralization, shape manipulation, etc.)\n","    \n","    \"\"\"\n","    \n","    def __init__(\n","            self, \n","            x, \n","            y, \n","            augmentation=None, \n","            preprocessing=None,\n","    ):\n","        self.x = x\n","        self.y = y\n","        \n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","    \n","    def __getitem__(self, i):\n","        # read data\n","        image = self.x[i]\n","        label = self.y[i]\n","        \n","        # apply augmentations\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, label=label)\n","            image, mask = sample['image'], sample['label']\n","        \n","        # apply preprocessing\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, label=label)\n","            image, mask = sample['image'], sample['label']\n","            \n","        return image, label\n","        \n","    def __len__(self):\n","        return self.x.shape[0]\n","    \n","    \n","class Dataloder(keras.utils.Sequence):\n","    \"\"\"Load data from dataset and form batches\n","    \n","    Args:\n","        dataset: instance of Dataset class for image loading and preprocessing.\n","        batch_size: Integet number of images in batch.\n","        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n","    \"\"\"\n","    \n","    def __init__(self, dataset, batch_size=1, shuffle=False):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.indexes = np.arange(len(dataset))\n","\n","        self.on_epoch_end()\n","\n","    def __getitem__(self, i):\n","        \n","        # collect batch data\n","        start = i * self.batch_size\n","        stop = (i + 1) * self.batch_size\n","        data = []\n","        for j in range(start, stop):\n","            data.append(self.dataset[j])\n","        \n","        # transpose list of lists\n","        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n","        \n","        return batch\n","    \n","    def __len__(self):\n","        \"\"\"Denotes the number of batches per epoch\"\"\"\n","        return len(self.indexes) // self.batch_size\n","    \n","    def on_epoch_end(self):\n","        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n","        if self.shuffle:\n","            self.indexes = np.random.permutation(self.indexes)   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yyD_fERaLQp3","colab_type":"code","colab":{}},"source":["import albumentations as A\n","\n","def round_clip_0_1(x, **kwargs):\n","    return x.round().clip(0, 1)\n","\n","# define heavy augmentations\n","def get_training_augmentation():\n","    train_transform = [\n","\n","        A.HorizontalFlip(p=0.5),\n","\n","        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n","\n","        A.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n","        A.RandomCrop(height=320, width=320, always_apply=True),\n","\n","        A.IAAAdditiveGaussianNoise(p=0.2),\n","        A.IAAPerspective(p=0.5),\n","\n","        A.OneOf(\n","            [\n","                A.CLAHE(p=1),\n","                A.RandomBrightness(p=1),\n","                A.RandomGamma(p=1),\n","            ],\n","            p=0.9,\n","        ),\n","\n","        A.OneOf(\n","            [\n","                A.IAASharpen(p=1),\n","                A.Blur(blur_limit=3, p=1),\n","                A.MotionBlur(blur_limit=3, p=1),\n","            ],\n","            p=0.9,\n","        ),\n","\n","        A.OneOf(\n","            [\n","                A.RandomContrast(p=1),\n","                A.HueSaturationValue(p=1),\n","            ],\n","            p=0.9,\n","        ),\n","        A.Lambda(mask=round_clip_0_1)\n","    ]\n","    return A.Compose(train_transform)\n","\n","\n","def get_validation_augmentation():\n","    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n","    test_transform = [\n","        A.PadIfNeeded(384, 480)\n","    ]\n","    return A.Compose(test_transform)\n","\n","def get_preprocessing(preprocessing_fn):\n","    \"\"\"Construct preprocessing transform\n","    \n","    Args:\n","        preprocessing_fn (callbale): data normalization function \n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose\n","    \n","    \"\"\"\n","    \n","    _transform = [\n","        A.Lambda(image=preprocessing_fn),\n","    ]\n","    return A.Compose(_transform)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJeP8VH5kFzt","colab_type":"text"},"source":["Costruzione modello"]},{"cell_type":"code","metadata":{"id":"fvSmZ724kA38","colab_type":"code","outputId":"a9e8c59f-6152-4418-e7a0-b91db567ff5e","executionInfo":{"status":"ok","timestamp":1587913812585,"user_tz":-120,"elapsed":717,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import segmentation_models as sm\n","\n","# segmentation_models could also use `tf.keras` if you do not have Keras installed\n","# or you could switch to other framework using `sm.set_framework('tf.keras')`"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Segmentation Models: using `keras` framework.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8fDpbHNpkJ5o","colab_type":"code","colab":{}},"source":["BACKBONE = 'efficientnetb3'\n","BATCH_SIZE = 8\n","LR = 0.0001\n","EPOCHS = 40\n","\n","preprocess_input = sm.get_preprocessing(BACKBONE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdfO0YcOkKjz","colab_type":"code","colab":{}},"source":["# define network parameters\n","n_classes = 40\n","activation = 'softmax'\n","\n","#create model\n","model = sm.Unet(BACKBONE, classes=n_classes, activation=activation, input_shape=(320,320,3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cd13xtLxkYHb","colab_type":"code","colab":{}},"source":["masksOccurency = np.arange(40)\n","for n in range(0, reshaped_labels.shape[0]):\n","  for i in range(0, reshaped_labels.shape[1]):\n","    for j in range(0, reshaped_labels.shape[2]):\n","      masksOccurency[reshaped_labels[n][i][j]] += 1\n","\n","labelList = []\n","for k in range(0, len(masksOccurency)):\n","  labelList.extend([k] * masksOccurency[k])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptivEFptkuVd","colab_type":"code","outputId":"80684a95-5c39-44bf-b284-f0058f0cf699","executionInfo":{"status":"ok","timestamp":1587913948596,"user_tz":-120,"elapsed":18183,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["from sklearn.utils import class_weight\n","class_weights = class_weight.compute_class_weight('balanced',np.unique(labelList),labelList)\n","#class_weights = dict(enumerate(class_weights)) dictionary for keras\n","class_weights"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2.1202915 , 0.52198782, 1.26013135, 2.23997597, 0.8550544 ,\n","       2.71960664, 0.61911924, 0.62490245, 0.90564414, 3.89039725,\n","       0.67618402, 1.58404131, 0.95352983, 0.27496577, 0.64053463,\n","       4.7687085 , 3.97269029, 2.47241303, 0.9518679 , 4.28935562,\n","       0.79128958, 2.13852942, 1.88429593, 2.50496736, 0.65771088,\n","       1.36489588, 4.78414347, 1.34214255, 0.83879863, 5.30564513,\n","       3.18227427, 0.91597808, 3.68038526, 0.90965939, 0.67399732,\n","       0.88036779, 0.78057669, 1.77479062, 0.21835131, 1.46587261])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"yuoV1EQVkw6b","colab_type":"code","colab":{}},"source":["# define optomizer\n","optim = keras.optimizers.Adam(LR)\n","\n","# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n","# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n","dice_loss = sm.losses.DiceLoss(class_weights=class_weights.tolist())\n","focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n","total_loss = dice_loss + (1 * focal_loss)\n","\n","# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n","# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n","\n","metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n","\n","# compile keras model with defined optimozer, loss and metrics\n","model.compile(optim, total_loss, metrics)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Hky3fONufbf","colab_type":"code","outputId":"448953c7-472c-414e-9d22-b28c7bf59e7d","executionInfo":{"status":"error","timestamp":1587916554010,"user_tz":-120,"elapsed":584,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":282}},"source":["# Dataset for train images\n","train_dataset = Dataset(\n","    x_train, \n","    y_train,\n","    augmentation=get_training_augmentation(),\n","    preprocessing=get_preprocessing(preprocess_input),\n",")\n","\n","# Dataset for validation images\n","valid_dataset = Dataset(\n","    x_val, \n","    y_val,\n","    augmentation=get_validation_augmentation(),\n","    preprocessing=get_preprocessing(preprocess_input),\n",")\n","\n","train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)\n","\n","# check shapes for errors\n","assert train_dataloader[0][0].shape == (BATCH_SIZE, 320, 320, 3)\n","assert train_dataloader[0][1].shape == (BATCH_SIZE, 320, 320, 40)\n","\n","# define callbacks for learning rate scheduling and best checkpoints saving\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\n","    keras.callbacks.ReduceLROnPlateau(),\n","]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/root/.local/lib/python3.6/site-packages/albumentations/augmentations/transforms.py:2029: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n","  warnings.warn('Using lambda is incompatible with multiprocessing. '\n"],"name":"stderr"},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-53a39c79c8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# check shapes for errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# define callbacks for learning rate scheduling and best checkpoints saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"NKmAFnlpNA8m","colab_type":"code","outputId":"27aa808d-401f-4d96-a8cd-c8f3f191e308","executionInfo":{"status":"ok","timestamp":1587916578407,"user_tz":-120,"elapsed":617,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_dataloader[0][1].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 195, 260, 40)"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"-Dnd1FuuCQpg","colab_type":"code","outputId":"8af311c0-9505-40c2-c4f4-673cc35cbff0","executionInfo":{"status":"ok","timestamp":1587913958964,"user_tz":-120,"elapsed":578,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(train_dataset)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["809"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"UqJP11ZnCTi_","colab_type":"code","outputId":"a381cbfa-e75b-4fe9-f9a7-9c939100c44a","executionInfo":{"status":"ok","timestamp":1587913960297,"user_tz":-120,"elapsed":591,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(valid_dataset)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["203"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"0pkZ5oBgrQm7","colab_type":"code","outputId":"2cd81632-b9e0-49a4-fdc2-fba189f92685","executionInfo":{"status":"error","timestamp":1587916298436,"user_tz":-120,"elapsed":946,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":458}},"source":["# train model\n","history = model.fit_generator(\n","    train_dataloader, \n","    steps_per_epoch=len(train_dataloader), \n","    epochs=EPOCHS, \n","    callbacks=callbacks, \n","    validation_data=valid_dataloader, \n","    validation_steps=len(valid_dataloader),\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/40\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-fc2eae185d4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m:  ConcatOp : Dimensions of inputs should match: shape[0] = [8,1536,14,18] vs. shape[1] = [8,816,13,17]\n\t [[node decoder_stage0_concat_5/concat (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_228252]\n\nFunction call stack:\nkeras_scratch_graph\n"]}]},{"cell_type":"code","metadata":{"id":"_BCjl15PJP0t","colab_type":"code","outputId":"d121c47b-391c-4a56-a5ab-8da3bb56d272","executionInfo":{"status":"error","timestamp":1587915623846,"user_tz":-120,"elapsed":1881,"user":{"displayName":"Andrea Scalvini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPXeWNqyQHU7lqzAh6lutN43ztPT6kRjC-mBXV=s64","userId":"07368200968117513143"}},"colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["model.fit(\n","   x=x_train,\n","   y=y_train,\n","   batch_size=16,\n","   epochs=100,\n","   validation_data=(x_val, y_val),\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 809 samples, validate on 203 samples\n","Epoch 1/100\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-ee66202ccfac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m:  ConcatOp : Dimensions of inputs should match: shape[0] = [16,1536,14,18] vs. shape[1] = [16,816,13,17]\n\t [[node decoder_stage0_concat_5/concat (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_228252]\n\nFunction call stack:\nkeras_scratch_graph\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ez6sLWAggXI2","colab_type":"text"},"source":["# Ricalcolo superfici normali da classificazione"]},{"cell_type":"code","metadata":{"id":"3gshBXstgX3Y","colab_type":"code","colab":{}},"source":["# Metodo che permette di ottenere la normale 3d per ogni pixel a partire dalla predizione della rete.\n","# INPUT\n","# netPrediction: shape = (H, W, 40). Predizione della rete di un'immagine. \n","#                Per ogni pixel la rete fornisce un vettore di 40 probabilità (1 per ogni centroide).\n","# codebook:      shape = (40,3). Lista delle 40 normali di riferimento (centroidi). codebook.shape = (40,3)\n","# triangoli:     shape = (N, 3). Triangolazione di delaunay; ogni elemento di questo vettore è una tripla di indici di centroidi, da utilizzare\n","#                nel codebook ottenuto con la clusterizzazione. (es. N = 67)\n","# OUTPUT\n","# norm:          shape = (H, W, 3). Normali 3d pixel per pixel dell'immagine.\n","\n","def decode(netPrediction, codebook, triangoli):\n","  h, w = netPrediction.shape[0:2]\n","  \n","  # for each triangle, get total prob\n","  tri_prob = np.dstack([np.sum(netPrediction[:,:,t], axis = -1) for t in triangoli]) \n","\n","  # get best tri\n","  best_tri = np.argmax(tri_prob, axis = -1)\n","  \n","  # get coefficients to most probable tri\n","  alphas = np.reshape([netPrediction[row, col, triangoli[best_tri[row, col]]] for row in range(h) for col in range(w)], (h, w, 3))\n","\n","  # sum -> 1\n","  alphas = np.divide(alphas, np.reshape(np.tile(np.sum(alphas, axis = -1), (1,3)), (h, w, 3)))\n","  alphas = np.reshape(np.tile(alphas, (1,1,3)), (h, w,3,3))  \n","  alphas = np.swapaxes(alphas, 2,3)\n","\n","  norm = np.multiply(alphas, codebook[triangoli[best_tri]])\n","  norm = np.sum(norm, axis = -2)\n","  \n","  return norm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3pQS4nYKgzK1","colab_type":"code","colab":{}},"source":["# chiamo la funzione di decodifica e ottengo la mappa delle normali 3d\n","netPrediction_3d = decode(netPrediction_mock, codebook, triangoli)\n","print(netPrediction_3d.shape)\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","#plt.figure('es. predizione decodificata')\n","plt.imshow(255/2 * netPrediction_3d)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWaT7KxAhNTD","colab_type":"text"},"source":["# Valutazione"]},{"cell_type":"code","metadata":{"id":"XAhRq-5NhbO5","colab_type":"code","colab":{}},"source":["# pixel wise evaluation\n","# INPUT\n","# prediction:   shape = (1,3) normale predetta dalla rete (ottenuta come codifica dalle label)\n","# ground_truth: shape = (1,3) normale effettiva\n","# OUTPUT\n","# l'errore in radianti tra la normale passata e quella effettiva\n","def eval_cosine(prediction, ground_truth):\n","  if np.sum(ground_truth) == 0 or np.sum(prediction) == 0:\n","    return 0\n","  cosine = np.dot(prediction, ground_truth) / (np.linalg.norm(prediction) * np.linalg.norm(ground_truth))\n","  if (cosine <= 1 and cosine >= -1):\n","    return np.arccos(cosine) \n","  return np.deg2rad(180) if cosine < -1 else 0\n","\n","# questa funzione va chiamata su tutti i pixel di tutte le immagini del test-set o validation set."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DffV2pi-hdJj","colab_type":"code","colab":{}},"source":["# TODO: effettuare la predizione su tutto il test-set (es: 30% del dataset \"normals_orig.npy\") con la rete. \n","# Dopodichè decodificare le predizioni per ogni immagine (label -> normali 3d) e possibilmente salvare in un file (es: \"prediction_test_decoded.npy\").\n","\n","# es. di test-set\n","idx_test = np.random.randint(0, N, int(0.3 * N))\n","normali_test = normali[idx_test]\n","\n","# NOTA: nel calcolo della metrica non vanno considerati i pixel rumorosi, che sono identificati dalla tripla [0,0,0].\n","tot_noised_pixel = np.where(np.sum(normali_test, axis = -1) == 0)[0].shape[0]\n","tot_pixel = normali_test.shape[0] * normali_test.shape[1] * normali_test.shape[2]\n","valid_pixel = tot_pixel - tot_noised_pixel\n","\n","print('Noised pixel / Tot pixel (%):', tot_noised_pixel / tot_pixel * 100)\n","print('Mean: ', np.rad2deg(np.sum(theta) / valid_pixel))\n","\n","# es: carico da file\n","# theta = np.load(\"prediction_test_decoded.npy\")\n","\n","# valuto la percentuale di errori angolari inferiori rispetto ad una delle seguenti soglie\n","soglie_errori = [11.25, 22.5, 30]\n","for th in soglie_errori:\n","  under_th = (np.where(theta < np.deg2rad(th))[0].shape[0] - noised_pixel) / valid_pixel * 100\n","  print('Threshold', str(th),  under_th)"],"execution_count":0,"outputs":[]}]}